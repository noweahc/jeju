{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    timestamp item corporation location  supply(kg)  price(원/kg)\n",
      "0  2019-01-01   TG           A        J         0.0          0.0\n",
      "1  2019-01-02   TG           A        J         0.0          0.0\n",
      "2  2019-01-03   TG           A        J     60601.0       1728.0\n",
      "3  2019-01-04   TG           A        J     25000.0       1408.0\n",
      "4  2019-01-05   TG           A        J     32352.0       1250.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나중에 test set에 넣어주어야 하기 때문에 만들어 둔 리스트\n",
    "\n",
    "# 월별 과일별 price 평균\n",
    "avg_m = [[] for _ in range(5)]\n",
    "\n",
    "# 년별 과일별 price 평균\n",
    "avg_y = [[] for _ in range(5)]\n",
    "\n",
    "# 요일별 과일별 price 평균\n",
    "avg_d = [[] for _ in range(5)]\n",
    "\n",
    "# location 과일별 price 평균\n",
    "avg_l = [[] for _ in range(5)]\n",
    "\n",
    "# corporation 과일별 price 평균\n",
    "avg_c = [[] for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_list_m = [i for i in range(1, 13)]\n",
    "range_list_y = [i for i in range(2019, 2024)]\n",
    "range_list_d = ['Monday', 'Tuesday', 'Wendsday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "range_list_l = ['J', 'S']\n",
    "range_list_c = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "\n",
    "# df, 처리할 컬럼, 컬럼값, 저장할 리스트\n",
    "def avg_cal (df, col, range_list, avg_list) :\n",
    "    for item_type in ['TG', 'CR', 'CB', 'RD', 'BC']:\n",
    "        for i in range_list :\n",
    "            condition = (df['item'] == item_type) & (df[col] == i)\n",
    "            avg_col = col + \"_avg\"\n",
    "            selected_rows = df[condition]\n",
    "            if not selected_rows.empty :\n",
    "                avg_value = selected_rows[avg_col].iloc[0]\n",
    "            else : \n",
    "                avg_value = 0\n",
    "            if item_type == 'TG':\n",
    "                avg_list[0].append(avg_value)\n",
    "            elif item_type == 'CR':\n",
    "                avg_list[1].append(avg_value)\n",
    "            elif item_type == 'CB':\n",
    "                avg_list[2].append(avg_value)\n",
    "            elif item_type == 'RD':\n",
    "                avg_list[3].append(avg_value)\n",
    "            elif item_type == 'BC':\n",
    "                avg_list[4].append(avg_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_train(df, whether) :\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['year'] = pd.to_datetime(df['timestamp']).dt.year\n",
    "    df['month'] = pd.to_datetime(df['timestamp']).dt.month\n",
    "    df['day'] = pd.to_datetime(df['timestamp']).dt.day\n",
    "    df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "    \n",
    "    if whether[0] == 1 :\n",
    "        df['month_avg'] = df.groupby(['month', 'item'])['price(원/kg)'].transform('mean')\n",
    "        avg_cal(df, 'month', range_list_m, avg_m)\n",
    "    \n",
    "    if whether[1] == 1 :\n",
    "        df['year_avg'] = df.groupby(['year', 'item'])['price(원/kg)'].transform('mean')\n",
    "        avg_cal(df, 'year', range_list_y, avg_y)    \n",
    "    \n",
    "    if whether[2] == 1 :\n",
    "        df['day_of_week_avg'] = df.groupby(['day_of_week', 'item'])['price(원/kg)'].transform('mean')\n",
    "        avg_cal(df, 'day_of_week', range_list_d, avg_d)  \n",
    "        \n",
    "    if whether[3] == 1 :\n",
    "        df['location_avg'] = df.groupby(['location', 'item'])['price(원/kg)'].transform('mean')   \n",
    "        avg_cal(df, 'location', range_list_l, avg_l) \n",
    "        \n",
    "    if whether[4] == 1 :\n",
    "        df['corporation_avg'] = df.groupby(['corporation', 'item'])['price(원/kg)'].transform('mean')      \n",
    "        avg_cal(df, 'corporation', range_list_c, avg_c)     \n",
    "        \n",
    "    if whether[5] == 1 :\n",
    "        df['sin_timestamp'] = np.sin(2 * np.pi * df['timestamp'].dt.dayofyear / 365)\n",
    "        df['cos_timestamp'] = np.cos(2 * np.pi * df['timestamp'].dt.dayofyear / 365)\n",
    "        \n",
    "    if whether[6] == 0 :\n",
    "        df.drop(columns=['month'], inplace=True)\n",
    "        \n",
    "    if whether[7] == 0 :\n",
    "        df.drop(columns=['day'], inplace=True)\n",
    "        \n",
    "    if whether[8] == 0 :\n",
    "        df.drop(columns=['year'], inplace=True) \n",
    "        \n",
    "    if whether[9] == 1:\n",
    "        df = pd.get_dummies(df, columns=['corporation'])\n",
    "    else:\n",
    "        df.drop(columns=['corporation'], inplace=True)\n",
    "    \n",
    "    if whether[10] == 1 :\n",
    "        df = pd.get_dummies(df, columns=['location']) \n",
    "    else :  \n",
    "        df.drop(columns=['location'], inplace=True)    \n",
    "        \n",
    "    if whether[11] == 1 :\n",
    "        df = pd.get_dummies(df, columns=['day_of_week']) \n",
    "    else :  \n",
    "        df.drop(columns=['day_of_week'], inplace=True)     \n",
    "    \n",
    "    df.drop(columns=['supply(kg)'], inplace=True)  \n",
    "    # df.drop(columns=['timestamp'], inplace=True) \n",
    "    df = pd.get_dummies(df, columns=['item']) \n",
    "                \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "binary_combinations = list(itertools.product([0, 1], repeat=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def modeling(df):\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    march_2019_data_indices = df[(df['timestamp'] >= '2019-03-01') & (df['timestamp'] < '2019-04-01')].index.tolist()\n",
    "    \n",
    "    col_list = df.columns\n",
    "    col_list = [col for col in col_list if col != 'price(원/kg)' and col != 'timestamp']\n",
    "\n",
    "    X = df[col_list]\n",
    "    Y = df['price(원/kg)']\n",
    "\n",
    "    X_train = df.loc[~df.index.isin(march_2019_data_indices), col_list]\n",
    "    Y_train = df.loc[~df.index.isin(march_2019_data_indices), 'price(원/kg)']\n",
    "\n",
    "    X_test = df.loc[df.index.isin(march_2019_data_indices), col_list]\n",
    "    Y_test = df.loc[df.index.isin(march_2019_data_indices), 'price(원/kg)']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model_lgbm = LGBMRegressor()\n",
    "    model_lgbm.fit(X_train_scaled, Y_train)\n",
    "    \n",
    "    predictions_lgbm = model_lgbm.predict(X_test_scaled)\n",
    "    \n",
    "    predictions_lgbm[X_test['day_of_week'] == 6] = 0\n",
    "    \n",
    "    mse_lgbm = mean_squared_error(Y_test, predictions_lgbm)\n",
    "    \n",
    "    return mse_lgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 23\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "1, MSE: 1094094.4280625181[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 44\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "2, MSE: 1094094.4295870159[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "3, MSE: 1019895.6135373642[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 50\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "4, MSE: 1019891.7270611438[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 41\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "5, MSE: 953785.7955258987[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 62\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "6, MSE: 953724.7710800108[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "7, MSE: 835915.2027208306[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 68\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "8, MSE: 835667.5685118268[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "9, MSE: 1161738.2291011724[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 50\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "10, MSE: 1162010.9190694413[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "11, MSE: 1080929.7817497652[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 56\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "12, MSE: 1080160.8078949717[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "13, MSE: 1062770.8378633617[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 68\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "14, MSE: 1062444.4401581944[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 53\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "15, MSE: 1000271.0191058927[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 74\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "16, MSE: 1001008.7025169601[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 55\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "17, MSE: 1120035.6001071243[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 76\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "18, MSE: 1119670.6970055879[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "19, MSE: 1048139.9960675752[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 82\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "20, MSE: 1046522.0319408262[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 73\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "21, MSE: 985127.3098838673[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 94\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "22, MSE: 981435.2309431904[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 79\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "23, MSE: 872613.7153113179[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 100\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "24, MSE: 859202.5261356356[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "25, MSE: 1248631.3137768665[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 82\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "26, MSE: 1260971.2190228235[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 67\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "27, MSE: 1134427.3253762526[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 88\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "28, MSE: 1171283.1435869352[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 79\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "29, MSE: 1118983.0348703533[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 100\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "30, MSE: 1142941.245442263[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 85\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "31, MSE: 1025887.9410255257[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 106\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 23\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "32, MSE: 1053535.823070329[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "33, MSE: 891112.7377455805[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "34, MSE: 891593.6698758199"
     ]
    }
   ],
   "source": [
    "min_values = [float('inf')] * 5\n",
    "whether_min_values = [[0] * 12] * 5\n",
    "count = 0 \n",
    "\n",
    "for whether in binary_combinations :\n",
    "    count = count + 1\n",
    "    df_ = df.copy() \n",
    "    total_df = preprocessing_for_train(df_, whether)\n",
    "    mse = modeling(total_df)\n",
    "    rmse = math.sqrt(mse)\n",
    "        \n",
    "    if rmse < max(min_values):\n",
    "        min_index = min_values.index(max(min_values))\n",
    "        min_values[min_index] = rmse\n",
    "        whether_min_values[min_index] = whether    \n",
    "        \n",
    "    print(f'\\r{count}, MSE: {mse}', end='', flush=True)\n",
    "    \n",
    "print(\"\\nTop 5 Minimum RMSE Values:\")\n",
    "for i, (min_value, whether_min_value) in enumerate(zip(min_values, whether_min_values), 1):\n",
    "    print(f\"{i}. RMSE: {min_value}, Corresponding Whether: {whether_min_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 201\n",
      "[LightGBM] [Info] Number of data points in the train set: 58188, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 1126.812143\n",
      "534907.835966243\n",
      "731.3739371663738\n"
     ]
    }
   ],
   "source": [
    "df_ = df.copy() \n",
    "mse = (modeling(preprocessing_for_train(df_, [1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1])))\n",
    "print((mse))\n",
    "print(math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "df_test.drop(columns=['ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       timestamp item corporation location\n",
      "0     2023-03-04   TG           A        J\n",
      "1     2023-03-05   TG           A        J\n",
      "2     2023-03-06   TG           A        J\n",
      "3     2023-03-07   TG           A        J\n",
      "4     2023-03-08   TG           A        J\n",
      "...          ...  ...         ...      ...\n",
      "1087  2023-03-27   RD           F        J\n",
      "1088  2023-03-28   RD           F        J\n",
      "1089  2023-03-29   RD           F        J\n",
      "1090  2023-03-30   RD           F        J\n",
      "1091  2023-03-31   RD           F        J\n",
      "\n",
      "[1092 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_avg_cal (df, col, range_list, avg_list) :\n",
    "    count1 = 0\n",
    "    for item_type in ['TG', 'CR', 'CB', 'RD', 'BC']:\n",
    "        count2 = 0\n",
    "        for i in range_list :\n",
    "            col_name = col + '_avg'\n",
    "            df.loc[(df['item'] == item_type) & (df[col] == i), col_name] = avg_list[count1][count2]\n",
    "            count2 = count2 + 1\n",
    "        count1 = count1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_test(df, whether) :\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['year'] = pd.to_datetime(df['timestamp']).dt.year\n",
    "    df['month'] = pd.to_datetime(df['timestamp']).dt.month\n",
    "    df['day'] = pd.to_datetime(df['timestamp']).dt.day\n",
    "    df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "    \n",
    "    if whether[0] == 1 :\n",
    "        put_avg_cal(df, 'month', range_list_m, avg_m)\n",
    "    \n",
    "    if whether[1] == 1 : \n",
    "        put_avg_cal(df, 'year', range_list_y, avg_y)\n",
    "    \n",
    "    if whether[2] == 1 :\n",
    "        put_avg_cal(df, 'day_of_week', range_list_d, avg_d)  \n",
    "        \n",
    "    if whether[3] == 1 :\n",
    "        put_avg_cal(df, 'location', range_list_l, avg_l) \n",
    "        \n",
    "    if whether[4] == 1 :\n",
    "        put_avg_cal(df, 'corporation', range_list_c, avg_c)     \n",
    "        \n",
    "    if whether[5] == 1 :\n",
    "        df['sin_timestamp'] = np.sin(2 * np.pi * df['timestamp'].dt.dayofyear / 365)\n",
    "        df['cos_timestamp'] = np.cos(2 * np.pi * df['timestamp'].dt.dayofyear / 365)\n",
    "        \n",
    "    if whether[6] == 0 :\n",
    "        df.drop(columns=['month'], inplace=True)\n",
    "        \n",
    "    if whether[7] == 0 :\n",
    "        df.drop(columns=['day'], inplace=True)\n",
    "        \n",
    "    if whether[8] == 0 :\n",
    "        df.drop(columns=['year'], inplace=True) \n",
    "        \n",
    "    if whether[9] == 1:\n",
    "        df = pd.get_dummies(df, columns=['corporation'])\n",
    "    else:\n",
    "        df.drop(columns=['corporation'], inplace=True)\n",
    "    \n",
    "    if whether[10] == 1 :\n",
    "        df = pd.get_dummies(df, columns=['location']) \n",
    "    else :  \n",
    "        df.drop(columns=['location'], inplace=True)    \n",
    "        \n",
    "    if whether[11] == 1 :\n",
    "        df = pd.get_dummies(df, columns=['day_of_week']) \n",
    "    else :  \n",
    "        df.drop(columns=['day_of_week'], inplace=True)     \n",
    "    \n",
    "    # df.drop(columns=['timestamp'], inplace=True) \n",
    "    df = pd.get_dummies(df, columns=['item']) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = preprocessing_for_train(df_, [1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1])\n",
    "\n",
    "col_list = df_.columns\n",
    "col_list = [col for col in col_list if col != 'price(원/kg)' and col != 'timestamp']\n",
    "\n",
    "X = df_[col_list]\n",
    "Y = df_['price(원/kg)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 193\n",
      "[LightGBM] [Info] Number of data points in the train set: 59397, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 1131.680674\n",
      "[3285.95804132    0.         3498.62893591 ...  517.8313954   517.8313954\n",
      "  520.40820926]\n"
     ]
    }
   ],
   "source": [
    "df_test = preprocessing_for_test(df_test, [1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1])\n",
    "\n",
    "df__ = df_test.copy()\n",
    "df__['timestamp'] = pd.to_datetime(df__['timestamp'])\n",
    "sunday_indices = df__[(df__['timestamp'].dt.dayofweek == 6)].index.tolist()\n",
    "\n",
    "col_list = df_test.columns\n",
    "col_list = [col for col in col_list if col != 'price(원/kg)' and col != 'timestamp']\n",
    "\n",
    "X2 = df_test[col_list]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X2)\n",
    "\n",
    "model_lgbm = LGBMRegressor()\n",
    "model_lgbm.fit(X_train_scaled, Y)\n",
    "\n",
    "predictions_lgbm = model_lgbm.predict(X_test_scaled)\n",
    "predictions_lgbm[sunday_indices] = 0\n",
    "print(predictions_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['answer'] = predictions_lgbm\n",
    "submission.to_csv('sample_submission2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
